{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: 'under the hood: training a digit classifier'\n",
    "description: 'fastai book chapter 4'\n",
    "date: \"2023-10-07\"\n",
    "date-format: iso\n",
    "image: under_the_hood_training_a_digit_classifier_thumbnail.jpg\n",
    "categories: [fastai, deeplearning, self-study]\n",
    "toc: true\n",
    "draft: true\n",
    "title-block-banner: false\n",
    "---\n",
    "[blog](../../blog.qmd) > under the hood: training a digit classifier\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![fastai book chapter 4 - jantxt - 2023](under_the_hood_training_a_digit_classifier_thumbnail.jpg){fig-align=\"left\" width=40%}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This my summary of chapter 4 from the book \"Deep Learning for Coders with fastai & PyTorch\".\n",
    "\n",
    " - questions - question about the chapter\n",
    " - key concepts - summarized key concepts of the chapter\n",
    "\n",
    "::: {.callout-note}\n",
    "## Links \n",
    "\n",
    "- Homepage: [fastai hompage](https://www.fast.ai/)\n",
    "- Online Book: [fastai online book](https://course.fast.ai/Resources/book.html)\n",
    "- Author: [jermey howard](https://jeremy.fast.ai/)\n",
    "- Author: [sylvain gugger](https://sgugger.github.io/)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "\n",
    "Questions about the chapter. \n",
    "\n",
    "[questions - chapter 4](subsite/under_the_hood_training_a_digit_classifier_questions.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Key Concepts\n",
    "\n",
    "Summarized key concepts ot this chapter.\n",
    "\n",
    "## NumPy Arrays and PyTorchTensors\n",
    "\n",
    "NumPy is the most widely used library for scientific and numeric programming in Python. A NumPy array is multidimensional table of data, with all items of the same type. Since that can be any type at all, they can even be arrays of arrays. \n",
    "\n",
    "Multidimensional tables are:\n",
    "\n",
    "- 1-dimensional tensor/array is called a vector\n",
    "```{python}\n",
    "[1,2,3]\n",
    "```\n",
    "\n",
    "- 2-dimensional tensor/array is called a matrix\n",
    "```{python}\n",
    "[[1,2], \n",
    "[3,4]]\n",
    "```\n",
    "\n",
    "- 3-dimensional or more tensor/array is called a tensor\n",
    "```{python}\n",
    "[[[1, 2],\n",
    "  [3, 4]],\n",
    " [[5, 6],\n",
    "  [7, 8]]]\n",
    "```\n",
    "\n",
    "Python is slow compared to other languages. NumPy and PyTorch wrap functions in other language (specifically C) to be much faster. So NumPy arrays and PyTorch tensors can finish computation many thousands of times fast than using pure Python.\n",
    "\n",
    "Pytorch vs Numpy:\n",
    "\n",
    "- Pytorch tensors are similar to numpy arrays, but can also be operated on GPU. Numpy arrays are mainly used in typical machine learning algorithms to support faster mathematical operations whereas pytorch tensors ar mainly used in deep learning which requires heavy matrix computations on the GPU. In addition PyTorch can automatically calculate derivatives of these fast computations on the GPU.\n",
    "\n",
    "::: {.callout-note}\n",
    "On of the most important skills when working with Numpy arrays and PyTorch tensors ist to learn how to effectively use the array/tensors APIs to run their processing and computations with the optimized fast functions they provide.\n",
    ":::\n",
    "\n",
    "## Broadcasting\n",
    "\n",
    "Broadcasting is a way to perform an operation between tensors that have similarities in their shapes. This is an important operation in deep learning. The common example is multiplying a tensor of learning weights by a batch of input tensors, applying the operation to each instance in the batch separately, and returning a tensor of identical shape. \n",
    "\n",
    "```{python}\n",
    "# Broadcasting Example 1: Vector + Scalar\n",
    "a = [1, 2, 3]        # Shape: (3,)\n",
    "b = 2                # Shape: ()\n",
    "# Result: [3, 4, 5]  # Shape: (3,)\n",
    "\n",
    "# Broadcasting Example 2: Matrix + Vector\n",
    "a = [[1, 2, 3],      # Shape: (2, 3)\n",
    "     [4, 5, 6]]\n",
    "b = [10, 20, 30]     # Shape: (3,)\n",
    "# Result:            # Shape: (2, 3)\n",
    "# [[11, 22, 33],\n",
    "#  [14, 25, 36]]\n",
    "```\n",
    "\n",
    "The rules of broadcasting:\n",
    "- each tensor must have a least one dimension - no empty tensor.\n",
    "- comparing the dimension sizes of the two tensors, going from last to first:\n",
    "    - each dimension must be equal of\n",
    "    - one of the dimension must be of size 1, or\n",
    "    - dimension does not exit in one of the tensors\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "### What is SGD?\n",
    "SGD is an optimization algorithm that updates model parameters to minimize the loss function. Think of it as helping the model learn from its mistakes by taking small steps in the right direction.\n",
    "\n",
    "### How it Works\n",
    "1. **Init**\n",
    "   - Random weights (training from scratch)\n",
    "   - Or pretrained weights (transfer learning)\n",
    "\n",
    "2. **Predict**\n",
    "   - Model makes predictions with current weights\n",
    "   - Initially poor predictions, improves over time\n",
    "\n",
    "3. **Loss**\n",
    "   - Compare predictions with actual targets\n",
    "   - Loss function measures how wrong predictions are\n",
    "   - Key difference from regular gradient descent: uses random subset of data (mini-batch)\n",
    "\n",
    "4. **Gradient**\n",
    "   - Calculate direction of steepest improvement\n",
    "   - Mountain analogy: \n",
    "     ```python\n",
    "     # Example: Simple gradient calculation\n",
    "     loss_gradient = 2 * (prediction - target)  # For MSE loss\n",
    "     ```\n",
    "\n",
    "5. **Step**\n",
    "   - Update weights using learning rate (Î·)\n",
    "   - Formula: `new_weight = old_weight - learning_rate * gradient`\n",
    "   ```python\n",
    "   # Example: Weight update\n",
    "   weights = weights - learning_rate * gradient\n",
    "\n",
    "## Gradient Descent vs Stochastic Gradient Descent\n",
    "\n",
    "### 1. Regular Gradient Descent\n",
    "- Uses ALL training data for each update\n",
    "\n",
    "### 2. Stochastic Gradient Descent\n",
    "- Uses SMALL RANDOM BATCHES of data\n",
    "\n",
    "### Simple Comparison\n",
    "Regular GD:\n",
    "\n",
    "  - One big step\n",
    "  - Very accurate\n",
    "  - Slow to compute\n",
    "  - More memory needed\n",
    "\n",
    "Stochastic GD:\n",
    "\n",
    "  - Many small steps\n",
    "  - Less accurate per step\n",
    "  - Fast to compute\n",
    "  - Less memory needed\n",
    "\n",
    "Regular GD: Reading an entire book before making notes\n",
    "\n",
    "SGD: Making notes after each chapter and adjusting as you go\n",
    "\n",
    "## Loss Function\n",
    "\n",
    "### What is a Loss Function?\n",
    "A loss function tells us how wrong our model's predictions are. \n",
    "\n",
    "Think of it as a \"wrongness score\":\n",
    "\n",
    "- Low score = Good predictions\n",
    "- High score = Bad predictions\n",
    "\n",
    "### How It Works in Training\n",
    "1. Model makes a prediction\n",
    "2. Loss function calculates how wrong it was\n",
    "3. Model tries to minimize this wrongness\n",
    "4. Repeat until predictions get better\n",
    "\n",
    "## Optimizers\n",
    "\n",
    "## Nonlinearity\n",
    "\n",
    "## Machine Learning Jargon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FastAi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
